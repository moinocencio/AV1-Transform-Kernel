\cleardoublepage
\chapter{Developed Architectures}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software Implementations}

The previous chapter presented some characteristics of the current state of \emph{libaom}'s \emph{Transform} stage which might compromise its performance, the most relevant being the unnecessary flexibility in the representation of cosine approximations.

In order to undertake these opportunities, and improve the overall encoder performance, new architectures for the studied stage were developed. The first approach was to study possible simplifications of the reference software, through the development and testing of alternative approaches for the provided functions.

The developed implementations tackled the forward \emph{DCT}, as it was the \emph{kernel} that would have the highest impact on encoder performance. As the \emph{IDCT} is shared between encoder and decoder, and due to the added complexity, no changes were done to this block, as it acts with accordance with the established standard, as mentioned previously. 

All the developed architectures and corresponding tests were written in $C$ programming language, as to maintain the simple integration into \emph{libaom}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Matrix Multiplication Implementation}

The first test was the application of the simplest integer \emph{DCT}, done by the multiplication of the input vector by a scaled up version of the transform matrix, $\mathbf{F}$, firstly shown in Equation \ref{eq:DCT2}. 

The original integer transform matrix is shown in Equation \ref{eq:matscale}.

\begin{equation} \label{eq:matscale}
    \begin{gathered}
        \mathbf{F}_{x,u} = \beta(u)\cos\left(\frac{(2x+1)u\pi }{2L}\right),\;0\leq u,x < L \\
        \Downarrow \\
        \mathbf{F} = \sqrt{\frac{2}{L}}  \begin{bmatrix}
            \sqrt{\frac{1}{2}}                                  & \sqrt{\frac{1}{2}}                                & \dots & \sqrt{\frac{1}{2}} \\
            \cos\left(\frac{\pi}{2L}\right)    & \cos\left(\frac{3\pi}{2L}\right) & \dots & \cos\left(\frac{(2(L-1)+1)\pi}{2L}\right) \\
            \vdots     & \vdots     & \ddots & \vdots       \\
            \cos\left(\frac{(L-1)\pi}{2L}\right)    & \cos\left(\frac{3(L-1)\pi}{2L}\right) & \dots & \cos\left(\frac{(2(L-1)+1)(L-1)\pi}{2L}\right) \\
        \end{bmatrix} 
    \end{gathered}
\end{equation}

As mentioned previously, the floating point coefficients bring a number of disadvantages on a hardware implementation, from increased calculation overheads, to encoder/decoder mismatches. 

In order to address these problems, a scale and rounding operation was performed, as shown in Equation \ref{eq:matscale}, where $K$ represents the number of bits of the scaled coefficients.

\begin{equation} 
    \nint*{\mathbf{F}_K}   = \nint*{2^K \mathbf{F}}
\end{equation}

However, due to the rectangular block sizes allowed in \emph{AV1}, the factor $\sqrt{\nicefrac{2}{L}}$ is not considered in the kernels themselves. Instead, the transformed outputs get scaled at a later stage. This way, the implemented transform matrix is

\begin{equation} \label{eq:matscale2}
    \begin{gathered}
        \nint*{\mathbf{F}_K}   = \nint*{2^K \sqrt{\frac{L}{2}}\mathbf{F}} \\
        = \nint*{2^K \begin{bmatrix}
                        \sqrt{\frac{1}{2}}                                  & \sqrt{\frac{1}{2}}                                & \dots & \sqrt{\frac{1}{2}} \\
                        \cos\left(\frac{\pi}{2L}\right)    & \cos\left(\frac{3\pi}{2L}\right) & \dots & \cos\left(\frac{(2(L-1)+1)\pi}{2L}\right) \\
                        \vdots     & \vdots     & \ddots & \vdots       \\
                        \cos\left(\frac{(L-1)\pi}{2L}\right)    & \cos\left(\frac{3(L-1)\pi}{2L}\right) & \dots & \cos\left(\frac{(2(L-1)+1)(L-1)\pi}{2L}\right) \\
                    \end{bmatrix} 
                }
    \end{gathered}
\end{equation}

This way, the  transformed outputs are calculated through

\begin{equation}
    \vec{\mathcal{G}} = \left[\nint*{\mathbf{F}_K} \vec{g}\right]>>K
\end{equation}

For an $L$ length vector, the calculation of the transformed vector implies $L^2$ additions and $L^2$ multiplications, which leads to the main disadvantage of such implementation. For larger vectors, this operation becomes too demanding in terms of memory and complexity.

One other negative aspect of such implementation is that, due to the variation of the transform matrix's coefficients, the obtained error in the rounding and scaling operation also varies with the vector size. The quantization\footnote{Here, quantization refers to the scaling and rounding operation, and not to the the \emph{Q} stage in an encoder.} \enlargethispage{-\baselineskip} error, $\Delta_K$, can be calculated as

\begin{equation}
    \Delta_K = \frac{\max{\left(\sqrt{\frac{L}{2}}\mathbf{F}\right)} - \min{\left(\sqrt{\frac{L}{2}}\mathbf{F}\right)}}{2^K}
\end{equation}

As it was proven in the previous Chapter that the number of bits in the cosine representation would not greatly impact the quality of the video, the developed architectures used 8 bits for the scaling operation, as to decrease the overhead of the implemented multiplications and shifts. The impact of this choice was evaluated at a later stage.

To evaluate the performance of this first implementation, a test was performed to measure and compare the elapsed time for both the described architecture, and the corresponding equivalent from \emph{aomenc}. This test injected a fixed sequence of 1 million input vectors into each of the \emph{DCT}'s,  measuring the elapsed cpu time in the operation. The results are in Table \ref{tab:dcttime}.

\begin{table}[!htpb]
    \centering
    \caption{Comparison of execution time between \emph{aomenc}'s DCT and the matrix multiplication implementation.}
    \begin{tabular}{ccc} \toprule
        \multirow{2}{*}{\textbf{Vector Size}} &     \multicolumn{2}{c}{\textbf{Execution Time (ms)}} \\
         &      \textbf{aomenc's} &      \textbf{MM} \\ \toprule
        \textbf{4} &    75 &       36 $(-52\%)$ \\ \hline
        \textbf{8} &    179 &      66 $(-63\%)$ \\ \hline
        \textbf{16} &   405 &      174 $(-57\%)$ \\ \hline
        \textbf{32} &   1039 &     686 $(-33\%)$  \\ \hline
        \textbf{64} &   3288 &     3590 $(+9\%)$  \\ 
        \bottomrule
    \end{tabular}    
    \label{tab:dcttime}
\end{table}

From these it is easily observable why the encoder's implemented transforms follow the \emph{butterfly} scheme. Although from sizes $4$ to $32$ the proposed implementation is faster than the current version of \emph{libaom}, the largest transform is slower. This factor, added to the error variation from the scaling operation makes this implementation quite damaging for the overall encoder performance, especially on a constant quality objective, as shown in Table \ref{tab:multresults}. Here, there are presented the timing results of an encoding test, where one encode was made with the standard \emph{aomenc}, the other had the proposed matrix multiplication \emph{DCT}'s. The test encoded the first 15 frames of the \emph{Parkrun} HD sequence, with two different quality objectives. After compression, the encoded video was decoded with \emph{aomdec}, calculating the PSNR of the output video.

\begin{table}[!htpb]
    \centering
    \caption{\emph{aomenc}'s encoding time with original vs implemented \emph{DCT}.}
    \begin{tabular}{cccc} \toprule
        \multirow{2}{*}{\textbf{cq-level}} & \multirow{2}{*}{\textbf{Measure}} &    \multicolumn{2}{c}{\textbf{Execution Time (ms)}} \\
        &   &   \textbf{Original} &      \textbf{MM} \\ \toprule
         \multirow{3}{*}{\textbf{60}}   & \textbf{Total time (s)}       & 466.5     & 530.8 \\
                                        & \textbf{\emph{Trans.} time (s)}    & 45.0      & 104.2 \\
                                        & \textbf{PSNR (dB)}            & 32.39     & 32.38 \\ \hline
         \multirow{3}{*}{\textbf{5}}    & \textbf{Total time (s)}       & 814.1     & 835.3 \\
                                        & \textbf{\emph{Trans.} time (s)}    & 60.4      & 98.4 \\
                                        & \textbf{PSNR (dB)}            & 34.88     & 34.86 \\                                        
         \bottomrule
    \end{tabular}    
    \label{tab:multresults}
\end{table}

As it is observable, to maintain a similar encoding quality, the encoder spends up to $13.8\%$ more time per encode, making such architecture unreliable for implementation on \emph{aomenc}.

Taking this into account, a new approach was employed, using the same \emph{butterfly} scheme as \emph{libaom}'s transforms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Alternative \emph{Butterfly} Implementation}

\emph{AV1}'s reference \emph{Transform stage} follows the aforementioned architecture for the DCT, in addition to expanding its use into the \emph{ADST}. In this scheme's publishing paper \cite{wen-hsiungchenFastComputationalAlgorithm1977}, the authors gave good reasons for the heavy adoption of this implementation, claiming that "\emph{The number of computational steps has been shown to be less than \nicefrac{1}{6} of the conventional DCT algorithm employing a 2-sided FFT}". 

This is achieved through a pipelined implementation of the previously shown matrix multiplication, where each stage is calculated as function of the previously calculated intermediary coefficients, as shown in Figures \ref{fig:intDCT} and \ref{fig:intADST}. Besides the reduction of complexity, this implementation also uses a fixed bank of \emph{cosine} coefficients (corresponding to \texttt{cospi} in \emph{libaom}), limited between $\cos\left(\nicefrac{\pi}{2}\right) < \alpha \leq \cos(0)$, i.e., $0 < \alpha \leq 1$ \footnote[1]{Zero is excluded from the set, as matrix $\mathbf{F}$ will not have any null element, for any $L$}. This way, the quantization error produced by the rounding and scaling operation is constant for all vector sizes.

To improve upon the reference \emph{DCT}, the developed architecture implemented a similar approach to the previous \emph{MM} architecture, using 8 bits for the scaling of the \emph{cosine} approximations. These values, $cos_{Apr}$, were generated using the same method as \texttt{cospi}, i.e.

\begin{equation}
    cos_{Apr} = \nint*{2^8 \cdot \cos\left(\frac{k\pi}{128}\right)}, \; 0 \leq k \leq 63
\end{equation}

Comparing the quantization step to the worst case for \emph{libaom}'s \emph{DCT}, with 10 bits

\nocite{shiImageVideoCompression2008}

\begin{gather}
    \Delta_{10} = \frac{1-0}{2^{10}} \approx 0.98\cdot10^{-3}\\
    \Delta_{8} = \frac{1-0}{2^{8}} \approx 3.9\cdot10^{-3}
\end{gather}
then it is possible to calculate the \emph{Mean Squared Quantization Error} through Equation \ref{eq:msequant}

\begin{equation} \label{eq:msequant}
    \begin{gathered}
        MSE_{Kq} = \frac{\Delta_K^2}{12} \\
        \Downarrow \\
        MSE_{10q} \approx 79.5 \cdot 10^{-9} \\
        MSE_{8q} \approx 1271.6 \cdot 10^{-9} = 16\cdot MSE_{10q} 
    \end{gathered}
\end{equation}

Although this value might seem discouraging, most of the error introduced in this stage is irrelevant once considered the error created by the encoder's \emph{Quantization} block. With that said, the impact of this approximation might compromise the encoder on higher quality (i.e., lower \emph{Quantizer}) objectives.

The implemented \emph{DCT} also deviates from the reference software in the multiplication of the cosine coefficients. The latter version calculated most of the intermediary functions with the \texttt{half\_btf} function (Equation \ref{eq:half_btf}), which, because the additional $2^{(K-1)}$, performs a \emph{rounding} (\gls{tld:round}) operation, as shown in Equation \ref{eq:alternhalfbtf}

\begin{equation} \label{eq:alternhalfbtf}
    \begin{gathered}
        \texttt{half\_btf(w0, in0, w1, in1, K)} \equiv \left(w_0\cdot in_0 + w_1\cdot in_1 + (1<<(K-1))\right)>>(K) \\
        \Downarrow \\
        \nint*{\frac{w_0\cdot in_0 + w_1\cdot in_1}{2^{K}}}
    \end{gathered}    
\end{equation}

On the implemented architecture, the rescaling did not include the additional factor, and was made just with the right shifting (\gls{tld:shiftr}) by 8 bits, corresponding to the \emph{flooring} operation (\gls{tld:floor}).

The constructed \emph{DCT} architecture underwent the first test as the previous implementation, giving origin to the results from Table \ref{tab:dcttime2}.

\begin{table}[!htpb]
    \centering
    \caption{Comparison of execution time between \emph{aomenc}'s DCT and the alternative \emph{butterfly} implementation.}
    \begin{tabular}{ccc} \toprule
        \multirow{2}{*}{\textbf{Vector Size}} &     \multicolumn{2}{c}{\textbf{Execution Time (ms)}} \\
         &      \textbf{aomenc's} &      \textbf{Alternative \emph{butterfly}} \\ \toprule
        \textbf{4} &    75 &       37 $(-51\%)$ \\ \hline
        \textbf{8} &    179 &      66 $(-63\%)$ \\ \hline
        \textbf{16} &   405 &      149 $(-63\%)$ \\ \hline
        \textbf{32} &   1039 &     355 $(-65\%)$  \\ \hline
        \textbf{64} &   3288  &    1362 $(-58\%)$  \\ 
        \bottomrule
    \end{tabular}
    \label{tab:dcttime2}
\end{table}

As shown, the developed architecture is, on average, $60\%$ faster for all vector sizes. This is due to the removal of the memory accessing overheads imposed by the access to the \texttt{cospi} array, as well the simplification of the performed operations. 

However, the reliability of this implementation depends on whether the encoder can maintain the desired quality with the increased error introduced by the quantization of the \emph{cosine} approximations. To verify this factor, the tests presented in Section \ref{ssec:performance} were repeated, once with the original encoder, and other with the encoder with the described alternative version of the \emph{DCT}. The obtained quality and timing results are presented in Figures \ref{fig:buttqual} and \ref{fig:butttime}, respectively. In the latter, the dotted lines correspond to time with the original encoder. The percentage above each bar represent the time difference taken by the alternative encoder, relative to the original.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{Sections/4DevelopedArchitecture/Figures/buttmultqual.eps}
    \caption{Obtained quality with original vs alternative \emph{DCT} implementation.}
    \label{fig:buttqual}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{Sections/4DevelopedArchitecture/Figures/buttmulttime.eps}
    \caption{Encoding time with original vs alternative \emph{DCT} implementation.}
    \label{fig:butttime}
\end{figure}

As shown, with the performed changes, the encoding time was, in average, reduced by $2.9\%$ for all quality objectives, while maintaining the output PSNR, making this a suitable \emph{DCT} implementation for an \emph{AV1} encoder. Although the performance improvement is rather diminishing once considered the full encoding cycle, it is a step toward a possible realtime encoding implementation.

However, the full impact of the applied changes is most noticeable on a hardware implementation. With the reduction of the cosine approximations, the memory used in the developed \emph{DCT}s is highly reduced. Considering $M_{\texttt{cospi}}$ as the number of bytes ($B$) used for storing the original cosine approximation vector,

\begin{equation}
    M_{\texttt{cospi}} = \frac{64\cdot (10+11+12+13+14+15+16)}{8} = 728\, B
\end{equation}
On the other hand, with the new implementation, only $64 \, B$  would be needed to store these approximations, corresponding to an $81\%$ reduction.

Nonetheless, both \emph{libaom}'s version and the developed could heavily benefit from parallelization, as is described in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hardware Implementations}

On the subject of hardware development, \glspl{fpga} have gained massive popularity within developers. Their ease of use, added to the applicability in certain designs as caused them to gain massive popularity for both prototyping and product implementation, since they present relatively smaller \emph{times-to-market} over other options. However, as usual, they come with compromises, namely the increased cost over specialized circuits, as well as the lower performances when compared to such \cite{FPGAVsASIC2016}.

Therefore, when it comes to hardware development, the best compromise would be to maintain the applicability of FPGAs in the prototyping stage, and easily migrate the tested designs into \glspl{asic}. This way, the first development stage could be done on a massively adaptable platform, and once concluded, the design could be implemented as a specialized design, without compromising its performance.

To achieve this, in recent years, there have been major developments in software platforms that allow for the synthesis of FPGA designs as ASICs, such as \emph{Cadence}'s \emph{Genus} or \emph{Synopsys' Design Compiler} \cite{GenusSynthesisSolution, DesignCompilerGraphical}. These softwares have been implemented in a variety of branches, when hardware development is necessary, namely, in video coding. 

This way, for the development of this work, the chosen development platform was \emph{Xilinx}'s \emph{Vivado}, due to the wide availability of its FPGAs, as well as for the wide support from its community. However, there should be kept in mind that to achieve the full performance of the developed designs, a specialized hardware implementation is desired. 

With an efficient algorithm for each of the supported vector sizes, the first objective was to develop a hardware architecture to implement each of the 1D \emph{DCT}s individually, and group them on a single block at a later stage. 

Two different architectures were developed. The first implements each of the \emph{DCT} blocks individually, while the latter uses sub-blocks of each \emph{DCT}, in order to achieve the final result. These architectures are explained in the following Sections.

With this approach, it was hopped to reach an architecture that englobed all the \emph{DCT} kernels, allowing to easily chose between each of them, depending on the desired choices made in the beginning of the transform stage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\emph{Individual 1D DCTs} Design}

The hardware implementations followed the same scheme as the corresponding software counterparts. By this it is meant that the flow of the input towards the output is done in individual and sequential stages. The main difference between these implementations is that in hardware, all of the intermediary signals within each stage are calculated in parallel. 

However, in order to achieve an efficient hardware implementation, some additional measures must be taken into consideration, mainly when considering the multiplication of signals by the cosine coefficients and re-scaling.

Consider an hypothetical operation performed in the software version, where two intermediary signals, \texttt{x1} and \texttt{x2}, get multiplied by some constant, added, and finally rescaled. In $C$, this operation is easily described in a single line of code, as shown in Figure \ref{fig:hardsoft}. However, to perform the same operation on an hardware descriptive language, some additional steps must be taken. The seemingly simple operation done in software must be deconstructed in various sequential steps, controlled by a clock signal. The operation shown in this Figure is repeated throughout the various \emph{DCT} implementations in hardware, making it the key to the development of the parallel architectures.

\begin{figure}[!htb]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/multHardware.tex}
    \caption{Comparison between software and hardware implementation of multiplication, sum and re-scaling.}
    \label{fig:hardsoft}
\end{figure}

Due to advances in \emph{VHDL} compilers and supporting libraries, both multiplication, shifts and additions are easily described, on a similar manner to a higher level language. Although on previous generations there would be some added benefits of implementing a multiplication by shifting and adding an input, as shown in Equation \ref{eq:multshift}, the improvements done in most recent years allow for similar architectures to be implemented, with less effort.

\begin{equation} \label{eq:multshift}
    \texttt{15 * x1} \equiv \texttt{(x1<<3) + (x1<<2) + (x1<<1) + x1}
\end{equation}

Taking these measures into consideration, the development of the 1D transforms becomes similar to all vector sizes. The software implementations are composed of alternating stages of simple summing operations, with more complex multiplying, sum and shift cycles. Therefore, the hardware counterparts are composed of three different blocks:

\begin{itemize}
    \item \emph{Summing Stages} where the inputs get added according to the previously shown butterfly schemes;
    \item \emph{Multiplier Stages}, which multiply the necessary inputs by the corresponding cosine coefficients;
    \item \emph{Shift Stages} that rescale the coefficients.
\end{itemize} 

Although these blocks are unique between transform sizes, and even within the same \emph{DCT}, the operations performed within are similar between all the vector sizes.

In order to ensure the correct pipelining of the \emph{Transform} process, each stage is controlled by an \emph{enable} flag, \texttt{en}, which signals the start of the block's process. Once it is concluded, the block outputs an indicator, \texttt{valOut}, that acts as the enable for the following stage, creating a daisy chain of stages. The last stage's \texttt{valOut} acts as the indication of the conclusion of the \emph{Transform} operation.

All blocks are controlled by the same \emph{clock} and \emph{reset} signals. The first triggers the internal processes on its ascending flank. The latter signals all internal registers to be put to 0 (its initial stage).

A simplified version of \textbf{DCT4}'s hardware implementation is represented in Figure \ref{fig:harddct4v1}. In here, the direction of the arrow represents if the corresponding signal is a \emph{input} or \emph{output}. The numbering of the output coefficients is done accordingly to the software implementation.

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/DCT4hardv1.tex}
    \caption{1D \textbf{DCT4} hardware implementation.}
    \label{fig:harddct4v1}
\end{figure}

To simplify the development of the hardware architectures, all signals and internal registers are represented, as this measure allows to easily interconnect the developed modules. Nonetheless, all modules are configurable through the modification of a \gls{genmap} parameter.

On a post-prototyping stage, to achieve an optimal utilization of resources, both inputs, outputs and internal signals should be shortened to the minimum length.

As a final measure to simplify the development process, the \emph{kernels} are implemented using the smaller sizes as a constituting block. As shown in Wen-Hsiung Chen's work \cite{wen-hsiungchenFastComputationalAlgorithm1977}, all transform sizes greater than $4$ englobe the same sequence of operations as the smaller counterparts, on one subset of its intermediary coefficients. This way, each of the smaller \emph{1D-DCT} blocks may be inserted into the size immediately above it.

As an example, \textbf{DCT8}'s hardware implementation is represented in Figure \ref{fig:harddct8v1}. There, it is observable that the architecture is similarly composed of the same blocks as the previous implementation. However, after the first summing stage, the first four intermediary coefficients are input into \textbf{DCT4}. 

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/DCT8hardv1.tex}
    \caption{Simplified 1D \textbf{DCT8} hardware implementation, with inclusion of \textbf{DCT4}.}
    \label{fig:harddct8v1}
\end{figure}

In the same manner, \textbf{DCT16} includes \textbf{DCT8}, which, as shown, also includes the four input version. This approach causes the smaller blocks to be repeated throughout the various larger architectures, making this approach highly inefficient from the chip's utilization standpoint. However, it brings the possibility to calculate each of the five \emph{DCT} sizes simultaneously, which is a highly desired characteristic on an encoder. As its objective is to encode each frame in the most efficient manner, the encoder tests various options, as to find the best for the current block. This behavior is present in the various stages, including the \emph{Transform}. Therefore, for an hardware encoder to be efficient, it must allow for the parallelization of encoding options.

One such implementation was implemented, as described in Figure \ref{fig:fullv1}.

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/fullDCTv1.tex}
    \caption{First version of the complete \emph{DCT} wrapper.}
    \label{fig:fullv1}
\end{figure}

Besides the previously shown 32 bit \texttt{dataIn}'s, \texttt{Clock}, \texttt{Reset} and \texttt{Enable}, this implementation adds an additional \texttt{Select} input.

As shown, this wrapper uses all the individual kernels independently, depending on the \textbf{Output Multiplexer} to conduct the correct output, according to the selected \emph{DCT}.

To validate this design, a VHDL test bench was built and simulated in \emph{Vivado}. It generates a vector of 32 bit integers, as well as the four control signals, injects them into the developed architecture, and receives the outputs. In Figure \ref{fig:v1timing} there is represented one of the timing tests made, where the selected size was 8. It shows the internal signals for the full architecture, as well as the selected block, \textbf{DCT8}.

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/v1timing.tex}
    \caption{Timing diagram for a test run on the first \emph{DCT} wrapper.}
    \label{fig:v1timing}
\end{figure}

From this test, it is observable that the desired functioning of the internal stages is achieved. The input gets sequentially pipelined through the various stages, getting a valid output 10 clock cycles after the enabling of the system. This behavior was also verified for the other vector sizes, although the delay until getting the output varies from \emph{DCT} to \emph{DCT}, between 6 clock cycles (\textbf{DCT4}) to 22 (\textbf{DCT64}).

%However, even though the correct functioning of the developed architecture is verified, its applicability is not verified yet. To draw conclusions, this design must first be synthesized into the FPGA which provides metrics as to the chip's utilization, power draw, timing characteristics, etc.

%The utilization results of this design's synthesis into an \emph{Artix 7} FPGA are presented in Table \ref{tab:v1results}. 

Considering these results, it is possible to calculate the necessary frequency of operation, in order for this architecture to maintain a given frame rate at a specific resolution.

To obtain the necessary number of transformation blocks to process each second, the desired frame rate must be multiplied by the number of $4 \times 4$ blocks within a frame of a given resolution (as to obtain the worst case scenario). Considering then that the presented results are referring to 1D vectors of length $L$, the number of clock cycles to process a 2D $L\times L$ block ($N_{2D}$) is given by

\begin{equation}
    N_{2D} = 2\cdot (L\cdot N_{1D})
\end{equation}

The necessary frequency of operation for frequencies between HD and 8K at 30 fps is represented in Table \ref{tab:freq30}.

\begin{table}[!htpb]
    \centering
    \caption{Necessary frequency of operation to obtain real-time encoding at 30 frames per second.}
    \begin{tabular}{cc} \toprule
        \textbf{Resolution}         & \textbf{Frequency (MHz)} \\ \toprule
        $\mathbf{1280\times 720}$   & 83 \\
        $\mathbf{1920\times 1080}$  & 187 \\
        $\mathbf{3840\times 2160}$  & 746 \\
        $\mathbf{7680\times 4320}$  & 2986 \\
        \bottomrule
    \end{tabular}    
    \label{tab:freq30}
\end{table}

With these results, several illations can be retrieved. For lower resolutions, this implementation can easily provide operable frame rates, as most devices can easily operate at 83 MHz. However, the same cannot be said of the necessary 3 GHz for 8K video. These results give an idea of the necessity to develop efficient architectures, that can take advantage of high parallelization in hardware, if high resolutions are desired. In the same manner, such architectures hardly could be implemented in FPGAs, since these usually present lower clock speeds than ASICs.

Such architectures would need to process several input vectors at once, as to process several transformation blocks simultaneously.

The developed hardware was then synthesized considering the \emph{Artix 7} FPGA family, obtaining the utilization results from Table \ref{tab:v1results}.

\begin{table}[!htpb]
    \centering
    \caption{First developed architecture's utilization in number of LUTs and Registers.}
    \begin{tabular}{ccc} \toprule
        \multirow{2}{*}{\textbf{DCT Size}} &     \multicolumn{2}{c}{\textbf{Utilization}} \\
         &      \textbf{Slice LUTs} &      \textbf{Slice Registers} \\ \toprule
        \textbf{4} &    1125  &       636  \\ \hline
        \textbf{8} &    2428  &       2087  \\ \hline
        \textbf{16} &   7103  &      5702  \\ \hline
        \textbf{32} &   19148  &     14257  \\ \hline
        \textbf{64} &   45996  &    34146  \\ \bottomrule        
        \textbf{Wrapper} & 75805  & 58370  \\
        \bottomrule
    \end{tabular}    
    \label{tab:v1results}
\end{table}

%As observable, this design uses more resources than what are available on the system's FPGA. This result, as mentioned previously was expected, due to the repetition of the smaller blocks on the different stages. The final architecture, effectively had five instances of the \textbf{DCT4}, four \textbf{DCT8}s, and so on. However, such an implementation could still be useful on a more capable kit, as will be discussed later in the following Section.

%Since this design is not synthesizable on the target hardware, no other results can be drawn, nor would they be applicable. However, on a more resourceful kit, this design could have been implemented, since there were no \emph{\gls{floorplaning}} errors.

%Nonetheless, this architecture allowed to draw conclusions as to the correct functioning of the \emph{DCT}'s internal stages, and served as the basis for the following architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\emph{Interdependent 1D DCTs} Design}

This next version's main objective was the reduction of necessary resources for FPGA implementation. The strategy taken was to avoid the repetition of the internal DCT stages, i.e., the final architecture should have a single instance of each of the previously shown internal stages throughout the entire design. 

To achieve this, each individual \textbf{DCTX} apart from \textbf{DCT4} was divided in sections. \textbf{DCTX\_P1} is composed of the first stage of each individual kernel, corresponding to the first summing/rotation. Correspondingly, \textbf{DCTX\_P2} groups all the following stages, from multiplications, sums and shifts, apart from the steps taken by the smaller \textbf{DCT$\nicefrac{X}{2}$} implementations. Figure \ref{fig:dct8iv} gives an exemplification of this sectioning, for the previous implementation of \textbf{DCT8}. All bocks are controlled by the same clock and reset signals. For simplification purposes, these signals wont be displayed in the following Figures.

\begin{figure}[!htb]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/DCT8divv2.tex}
    \caption{Exemplification of the individual kernel's division for the second implementation.}
    \label{fig:dct8iv}
\end{figure}

In Appendices \ref{app:dct81} and \ref{app:dct82} there are presented the implemented VHDL descriptions for both \emph{DCT8} blocks. The following blocks follow the same architecture, varying on the multiplication coefficients and number of stages.

It is important to note that \textbf{DCT8\_P2} inputs and \texttt{en} are hardwired to the bottom four outputs and \texttt{valOut} of \textbf{DCT8\_P1}, respectively. Similar connections are done throughout the whole architecture, as each of the \textbf{DCTX\_P2} stages will always process outputs from the corresponding \textbf{P1} stages. However, the latter may be injected with the systems \texttt{dataIn}'s, or with intermediary coefficients from previous stages. %This behavior is shown in later in this Section.

Depending on the selected size, the input data is injected into one of the \textbf{P1} stages or directly into \textbf{DCT4}. The interconnection of the internal blocks, and corresponding flow of the intermediary coefficients is dealt by a arrangement of multiplexers, that differ from the previous implementation. In this case, this block has a much higher input/output count, as it must control which signals go into each stage, to generate the correct final coefficients as if the \texttt{dataIn}'s passed through a single \textbf{DCT} block. Besides the coefficients, also the intermediary enable signals are dependent of the central unit, since the interconnection of the \texttt{valOut/en} pairs is dependent on the selected vector size.

A simplified version of the achieved architecture is represented in Figure \ref{fig:fullv2}, where no clock or reset signals are represented. As mentioned previously, the system is composed of a single instance of each of the intermediary stages. This makes the achieved architecture similar to a single \textbf{DCT64} block from the first version. In other words, the largest block from the previous implementation had the same set of internal blocks as the newer \emph{Wrapper}, but instantiated in a different manner. In this case, since all the internal intermediary points are accessible by the \textbf{Coefficient Multiplexer}, all the smaller transformations can be calculated with a single kernel of size 64. Figure \ref{fig:v2datapath} demonstrates this behavior. Depending on the selected vector size, the data is sequenced by different blocks, as shown by the different colored lines. The dotted lines represent the enable signals which will be activated at some point of the transformation process.

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/v2datapath.tex}
    \caption{Flow of \texttt{dataIn} according to the selected vector size.}
    \label{fig:v2datapath}
\end{figure}

\begin{landscape}
    \vspace*{\fill}
    \begin{figure}[!htbp]
        \centering
        \input{Sections/4DevelopedArchitecture/Figures/fullDCTv2.tex}
        \caption{Simplified architecture of the second version of the full \emph{DCT} wrapper.}
        \label{fig:fullv2}
    \end{figure}
    \vspace*{\fill}        
\end{landscape}


This architecture was subjected to the same test bench as the previous version. In Figure \ref{fig:v2timing} it is represented one of these tests, where 16 was the selected size. In this case, there are no internal signals represented, only being shown the most relevant of the ones accessible by the central multiplexer, for this case. 

\begin{figure}[!htbp]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/v2timing.tex}
    \caption{Timing diagram for a test run on the second \emph{DCT} wrapper.}
    \label{fig:v2timing}
\end{figure}

As it is observable, the central block handles the direction of intermediary coefficients and enable signals between the internal blocks, according to the selected size. The final stage taken by it is the re-organization of each of the \textbf{P2} stages' outputs.% The timing performance results are presented at a later in this section.

%Given the desired behavior was achieved, the architecture was synthesized and implemented on the hardware kit, being the system's characteristics presented in Table \ref{tab:v2results}.

Similarly to the previous architecture, the current was also synthesized considering an \emph{Artix 7} FPGA, giving origin to the utilization results from Table \ref{tab:v2results}.

\begin{table}[!htpb]
    \centering
    \caption{Second developed architecture’s utilization in number of LUTs and Registers.}
    \begin{tabular}{ccc} \toprule
        \multirow{2}{*}{\textbf{Block}}     & \multicolumn{2}{c}{\textbf{Utilization}}              \\
                                            & \textbf{Slice LUTs}      & \textbf{Slice Registers}   \\ \toprule
        \textbf{DCT4}                       & 1077                     & 507    \\
        \textbf{DCT8\_P1}                   & 709                      & 257    \\
        \textbf{DCT8\_P2}                   & 1064                     & 717    \\
        \textbf{DCT16\_P1}                  & 1285                     & 513    \\
        \textbf{DCT16\_P2}                  & 3860                     & 2150   \\
        \textbf{DCT32\_P1}                  & 3064                     & 1025   \\
        \textbf{DCT32\_P2}                  & 9090                     & 5624   \\
        \textbf{DCT64\_P1}                  & 6123                     & 2049   \\
        \textbf{DCT64\_P2}                  & 22344                    & 14000  \\ \bottomrule
        \textbf{Wrapper}                    & 50039                    & 32352  \\
        \bottomrule
    \end{tabular}    
    \label{tab:v2results}
\end{table}

As observable, this implementation occupies $66\%$ of the area occupied by the former, approximately corresponding to the occupation taken by \textbf{DCT64}. This makes the current version more efficient from the utilization standpoint. However, it can only calculate one \emph{DCT} size at a time, as all blocks are interdependent. This makes such implementation undesirable for an encoder, since no parallelization of transformation options is achieved and, therefore, to obtain the five \emph{DCT} outputs for a given input vector, the architecture would need to calculate each size sequentially, bringing the total number of clock cycles to

\begin{equation}
    N_{Total} = 6+10+14+18+22 = 70
\end{equation}
while to obtain the same results with the previous implementation there would only be needed 22.

If real time encoding is not necessary, such implementation would be most suitable for mobile applications, where size and consumed power are key requirements, as this version would perform better than the previous implementation on both of these aspects.

%From these results, it is possible to conclude that the achieved implementation is applicable in the target hardware, given that the architecture occupies approximately 80\% of the available resources. And, as mentioned previously, the obtained architecture's occupation is similar to \textbf{DCT64}'s, as the additional 4043 LUTs taken by this implementation can be justified by the more complex redirecting unit.

%Besides this, it is also possible to get an estimation of the power consumed during operation. As to the system's maximum operating frequency, it can be approximated using the \gls{wns}. This measure represents the lowest delay to meet the design's requirements, i.e., taking the system's longest clock propagation path, if it were increased by WNS, the timing constraints would still be met. Therefore, considering $T$ as the current operating clock's period, an estimation of the maximum operating frequency may be calculated by Equation \ref{eq:maxf}.

%\begin{equation} \label{eq:maxf}
%    f_{Max} = \frac{1}{T-WNS} = \frac{1}{10\cdot 10^{-9} - 0.188\cdot 10^{-9}} = 101.9\,MHz
%\end{equation}

%As seen, the design is already close to the maximum operating frequency, for the provided objectives. Hence, if a different set of design constraints were to be provided, this frequency might increase.

%Taking the obtained results, the following step was the integration of the design on a system more resembling of a complete encoder.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\emph{Microblaze} Integration} \label{sec:microblaze}

Considering the system presented on Figure \ref{fig:basicenc} (page \pageref{fig:basicenc}, \emph{Simplified Basic Encoder Model}), it would be safe to assume that, if it were to be implemented in hardware, the \textbf{Control Unit} would be implemented on a generic CPU, controlling the surrounding blocks, e.g. the developed \emph{DCT} kernels. Such an architecture would allow for a complex software, such as \emph{libaom}, to be massively simplified, as most of the complex calculations would be run on specialized co-processors, while leaving the system's controlling and decision-making processes to be run on a central unit.

In order to prove the applicability of the developed architectures in such system, the designs were tested on a \emph{Digilent}'s Nexys 4 hardware kit, equipped with an \emph{Artix 7} FPGA, XC7A100T-1CSG324C. This board is presented in Figure \ref{fig:nexys4}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\figwidth]{Sections/4DevelopedArchitecture/Figures/nexys-4-0.png}
    \caption[Nexys 4 hardware kit.]{Nexys 4 hardware kit \cite{NexysArtix7FPGA}.}
    \label{fig:nexys4}
\end{figure}

With the hardware chosen, the first step would be to synthesize the developed architectures to the desired FPGA. This operation provides numerous relevant data, such as the percentage of utilization, the estimated power drawn or some timing characteristics. 

Referring to the first implementation of the \emph{DCT} kernel, the Synthesis to the current hardware is not possible, since the design occupies $119.57\%$ of the available slice LUTs. However, on a more capable kit, this design could be tested.

However, as the second implementation uses only $79.93\%$, and no other \emph{floorplaning} errors occurred, the Synthesis and Implementation were successful, and therefore, \emph{Vivado} provides the mentioned estimations. With an operating frequency of 100 MHz, the obtained power draw is 50 mW. With the \gls{wns}, it is also possible to calculate the system's maximum operating frequency. This measure represents the lowest delay to meet the design's requirements, i.e., taking the system's longest clock propagation path, if it were increased by WNS, the timing constraints would still be met. Therefore, considering $T$ as the current operating clock's period, an estimation of the maximum operating frequency may be calculated by Equation \ref{eq:maxf}.

\begin{equation} \label{eq:maxf}
    f_{Max} = \frac{1}{T-WNS} = \frac{1}{10\cdot 10^{-9} - 0.188\cdot 10^{-9}} = 101.9\,MHz
\end{equation}

To consider a full \emph{libaom} integration in the focus of this work, with the target hardware, would be too demanding both in terms of complexity and achievability. Since a single block of the encoder occupies most of the available resources, the integration of the following processes would certainly need a highly capable hardware kit. 

Nonetheless, a simpler architecture can still be implemented, as to test the applicability of the developed architecture on a full encoding system. To do this, the encoder's central unit would need to be instantiated on the hardware kit on the shape of a \gls{mcu}, alongside the developed kernels. The latter also needed to be adapted for the communications between it and the MCU.

To accomplish this, \emph{Vivado} provides a wide set of tools to easily achieve integration of a design described in VHDL with a generic processor, efficiently obtaining a hybrid design between the developed hardware and a software algorithm, easily developed in \emph{C}, as an example.

Therefore, two separate tasks were conducted. First was the preparation of the \emph{DCT Wrapper}, and finally the development of a complete system.

%%%%%%%%%%
%\paragraph{AXI4 Interface}

Taken the architecture from Figure \ref{fig:fullv2}, the first step was the creation of a custom block with an \emph{AXI4} interface. This data-transfer protocol is heavily used in \emph{Xilinx}'s tools and \emph{ARM}'s processors.

It is based on a generic \emph{Master-Slave} interaction with two separate channels. The \emph{Address} channel includes the location of the register to read/write from, as well as the necessary control signals. The \emph{Data} channel transports the information coming from the \emph{Slave} (read cycle) or \emph{Master} (write cycle). 

\emph{Vivado} allows for three different versions of \emph{AXI4}:

\begin{itemize}
    \item \textbf{AXI4} implements a highly customizable memory mapped interface, indicated for complex applications;
    \item \textbf{AXI4-Lite} is a simplified version of the former, keeping the memory mapped communications;
    \item \textbf{AXI4-Stream} implements a streaming protocol, allowing a high throughput.
\end{itemize}

For the development of this work, the second protocol was chosen, as it allowed to prove the efficiency of the developed design on a full system, while keeping a low complexity. However, if a complete encoder were to be constructed, the stream configuration would be more adequate. Since the \emph{Wrapper} would need to process large amounts of coefficients each second, the interface from which it received the input vectors would need to be able to provide the necessary throughput.

The integration of the \emph{Wrapper} with the interface constituted of connecting each of the \texttt{dataIn}s to one of the interface's 32 bit writable registers. This way, all the inputs are accessible by the controller when performing a writing operation. As to the outputs, the same addresses were attributed to the corresponding \texttt{dataOut}s, allowing to read the calculated coefficients. The \texttt{enable}, \texttt{reset} and \texttt{Select} signals were all connected to the same write register, as these signals occupy 5 bits in total.

Besides the \emph{DCT} kernel, three counters were implemented. Each of these were tasked with counting the write, read and transformation clock cycles. The first two are controlled by activating/deactivating bits from the control register, while the other is controlled by the \texttt{enable} and \texttt{validOut} signals from the kernel. The output values are also accessible to the microcontroller, making possible the monitoring of the whole cycle. And as the timers are implemented in hardware, there are not as significant overheads as if the operations were timed with a software counter.

Figure \ref{fig:axi4int} shows a simplified version of the kernel's integration with the \emph{AXI4} Interface. \textbf{DCT Timer}'s output is concatenated with \texttt{validOut} since it does not need the full 32 bits. The internal blocks are all driven with the same system clock.

\begin{figure}[!htb]
    \centering
    \input{Sections/4DevelopedArchitecture/Figures/v2AXI4.tex}
    \caption{Simplified description of \emph{DCT Wrapper} with AXI4-Lite interface.}
    \label{fig:axi4int}
\end{figure}

The additional structures raised the number of used Look Up Tables to 50461 ($79.59\%$ of \emph{Artix 7}) and Registers to 34686 ($27.35\%$).

%%%%%%%%%%
%\paragraph{Microblaze}

With this block, a separate design was created. To it was added a \emph{Microblaze} microcontroller, which instantiates a \gls{risc} soft core processor, specialized for use with \emph{Xilinx}'s products \cite{MicroBlazeProcessorReference2019a}. 

Specifying the desired interfaces, \emph{Vivado} automation routines handle most of the necessary connections. To the \emph{Microblaze} and \emph{DCT Wrapper}, was also added a \gls{uart} connection for debugging purposes, giving origin to the block design from Figure \ref{fig:blockdes}. The addition of the microcontroller and peripherals added 1512 LUTs and 1387 registers, bringing to a final $81.98\%$ and $28.45\%$ utilization, respectively.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{Sections/4DevelopedArchitecture/Figures/DCTCop.png}
    \caption{Block design generated by \emph{Vivado} for integration of \emph{DCT Wrapper} with \emph{Microblaze}.}
    \label{fig:blockdes}
\end{figure}

With the finalized design, some tests were developed in software, to be run on the \emph{Microblaze}. The interaction between it and the \emph{DCT Wrapper} is done with read and write routines provided by \emph{Xilinx}'s \gls{sdk}. These access the addresses attributed to the peripherals during \emph{Vivado}'s Implementation process.

The first test revolved around the verification of the calculated of coefficients. The processor injected a sequence of input vectors into the co-processor, and compared the obtained results with the software's. This short test proved successful, as the hardware implementation gave the same result as the software version for all vector sizes.

Being proven the correct calculation of the transformed coefficients, the final test was to verify the timing performances of the system. For this, a small program was written, where a vector with the same length as the desired \emph{DCT} would be loaded into the co-processor, this would be activated, and the same length transformed vector would be read back.

The most relevant result to measure was the number of clock cycles to calculate the transformed coefficients, and verify the accordance with the VHDL test bench. The read and write processes should only be seen as references, as they are highly dependent on the used interface. This way, in Table \ref{tab:axi4time} there are presented the separate timing results for the three different processes measured, both in number of clock cycles, as well as the time duration at the calculated maximum frequency. 

\begin{table}[!htpb]
    \centering
    \caption{Timing results for the \emph{Microblaze} integration design.}
    \begin{tabular}{ccccccc} \toprule
        \multirow{2}{*}{\textbf{Size}}   & \multicolumn{3}{c}{\textbf{Number of Clock Cycles}$\mathbf{(T_{@101.9MHz}(ns))}$}                      \\
                                         & \textbf{Write} & \textbf{Transform} & \textbf{Read}  \\ \toprule
        \textbf{4}                       & 510 $(5005)$   & 6 $(59)$           & 462 $(4534)$   \\
        \textbf{8}                       & 902 $(8852)$   & 10 $(98)$          & 806 $(7910)$   \\
        \textbf{16}                      & 1686 $(1646)$  & 14 $(137)$         & 1494 $(14661)$ \\
        \textbf{32}                      & 3254 $(31933)$ & 18 $(177)$         & 2870 $(28165)$ \\
        \textbf{64}                      & 6390 $(62709)$ & 22 $(216)$         & 5622 $(55172)$ \\
        \bottomrule
    \end{tabular}
    \label{tab:axi4time}
\end{table}

Taking the transformation times, it is possible to calculate the hypothetical throughput of the developed architecture on the current hardware. %This is a measure of the highest frame rate this architecture could process at a given resolution. 
This is calculation is done with a similar process as the one of Table \ref{tab:freq30}, but in reverse. Given the maximum estimated frequency, the maximum possible frame rate is estimated. In Table \ref{tab:maxfps}, these results are presented, considering all square block sizes.

%On a video encoder, each residue block will suffer several transformations, horizontal and vertically. Given that the presented results are referring to vector transformations, the hypothetical time for transforming a 2D block must be calculated. With the obtained results, the maximum possible frame rate of some of the most common resolutions was calculated, considering only blocks of the the corresponding size were used, giving origin to Table \ref{tab:maxfps}.

\begin{table}[!htpb]
    \centering
    \caption{Maximum frame rate for a given resolution, considering fixed square transformation blocks, on the Nexys 4 implementation.}
    \begin{tabular}{ccccc} \toprule
        \multirow{2}{*}{\textbf{Block Size}}   & \multicolumn{4}{c}{\textbf{Resolution}}                      \\
                                         & $\mathbf{1280\times 720}$    & $\mathbf{1920\times 1080}$  & $\mathbf{3840\times 2160}$  & $\mathbf{7680\times 4320}$  \\ \toprule
        $\mathbf{4\times 4}$                       & 37                      & 16                   & 4                    & 1  \\
        $\mathbf{8\times 8}$                       & 44                      & 20                   & 5                    & 1  \\
        $\mathbf{16\times 16}$                      & 63                      & 28                   & 7                    & 2  \\
        $\mathbf{32\times 32}$                      & 98                      & 44                   & 11                   & 3  \\
        $\mathbf{64\times 64}$                      & 161                     & 71                   & 18                   & 4  \\
        \bottomrule
    \end{tabular}    
    \label{tab:maxfps}
\end{table}

As seen, the constructed architecture, on the current hardware, is not capable of processing high resolution video at usable frame rates. For HD video, it can maintain 30 fps for all block sizes. However, on higher resolutions, this throughput cannot be maintained, in order to obtain real time usability. 

Other works capable of providing such performances have been published, mostly for the HEVC standard (see references \cite{vayalilEfficientASICDesign2016,meherEfficientIntegerDCT2014,m.HighPerformanceInteger2017}). These results should justify the choices made by the developers, since the proclaimed clock frequencies are obtained with \gls{vlsi} circuits, and FPGA implementations, when present, are only used for validation purposes.

In addition, it should be kept in mind that the presented results represent a \emph{best case} scenario, since they do not take into consideration any other delays from the encoding process.

Nonetheless, the integration of the developed architecture with the \emph{Microblaze} should prove the applicability of the first on a full encoding system, even though the maximum performance of the system is not known, as it could be achieved through synthesis to ASIC.

%With the second version of of the \emph{DCT Wrapper}, the utilization of hardware is heavily optimized, as there are no repeated stages throughout the design. However, this makes  parallel transformations of different vector sizes impossible. Since, at least, \textbf{DCT4} will always be occupied with one of the encoding options, and as all other transform sizes need this stage to calculate its output, there is not the possibility to calculate two different transform sizes at the same time.

%However, on the decoding process, there is no need for parallelization of the Transform stage, as all the encoding options were already chosen. This way, an architecture following the same implementation would be appropriate for a decoder, where low power and size are highly desired characteristics.

%On the other hand, the first version of the complete kernel had all \emph{DCT} blocks individualized, making the transformation of different sized vectors possible. As mentioned before, this makes this implementation better suited to be applied on an encoder, as it allows to test various configurations at the same time, and quickly evaluate the most adequate. However, for this to be achieved, some modifications would need to be made, as only one set of output vectors can be retrieved on the current implementation.


\clearpage
\printbibliography[heading=subbibliography]
\addcontentsline{toc}{section}{References}