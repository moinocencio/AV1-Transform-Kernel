\cleardoublepage
\pagenumbering{arabic}
\chapter{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Motivation}

%\todo[inline,color=green!40]{*Spark of video research}
%\todo[inline,color=green!40]{*Necessity for video compression}
%\todo[inline,color=green!40]{*Higher compression ratios demand higher complexity}
%\todo[inline,color=green!40]{*High video consumption on mobile devices}
%\todo[inline,color=green!40]{*Dedicated hardware solutions}

Since the spark of television research in 1887, a tremendous investment has been put into increasing the quality of images, cameras and screens that display them \cite{schubinWhatSparkedVideo2017}.

In the early years of mechanical television, this desire was pursued by making changes to the \textit{Nipkow} disks, up to the decline of the mechanical TV, around the 1930's. The consequential rise of all-electronic TVs started with the capture of images with the same cathode tubes put into \glspl{CRT}, with broadcasts of the live analog recordings, since there were no available methods of storing images, up to 1955, with the development of the open-reel magnetic tape \cite{jacobsBriefHistoryVideo}.

The evolution of \Gls{CMOS} technologies however, led to the downfall of cathode ray tubes, and to the rise of image capture to a digital sensor, that allowed better image captures and lower demands in terms of physical storage space. However, with the desire for higher fidelity video, the quantity of information captured also increased. Whether by increasing the sensor resolution, color bit depth or frame rate, the captured video sequences have increased its size throughout the years. For instance, for a video of $640 \times 360$ (considered as a low resolution), at 30 \gls{fps}, considering each captured color (\GLS{rgb}
\todo[size=\tiny]{Falar dos color spaces mais à frente?}
) is represented with 8 bits, there is approximately 166 Million bits per second (Mbps) of captured information. This means that a short 5 minute video would occupy more than 6 Giga Bytes (GB) of memory. This aspect gets more severe once higher resolutions are considered. For newer standards such as 4K \Gls{UHD} ($3840 \times 2160$) or 8K UHD ($ 7680 \times 4320$), under the same conditions, a ten minute video would occupy 448 GB and 1792 GB of raw data, respectively.

To further aggravate the situation, video consumption got massively adopted on the average consumer level, and continues to grow, both in the average number of hours watched by users and in the resolutions of the video, making the bandwidth used on the visualization of video footage the highest between all other application. With the development of higher video sizes, increase of the average number of connected devices per user and overall market expansion through the number of consumers, this margin will continue to grow \cite[Trends~1~\&~4]{CiscoVisualNetworking}.

This problem has led to the introduction of a new concept: \emph{Video Compression} \footnote{Also called \textit{Video Coding}.}, which is the process of reducing the size of a video sequence, while still maintaining its playback capabilities. The \Gls{codec} takes advantage of redundant information present on the raw data to reduce the size of the video, without heavily modifying the original picture or its quality. 

The first form of video compression, \gls{interlacing}, dates from 1940, and was purely analog. This solution was introduced with the intent of reducing the necessary broadcasting bandwidth for old CRTs, without decreasing the displayed fps. And even though this technique has been implemented over more than seventy years, it has proven to be so efficient that most TV channels today still use interlaced broadcasting.

However, analog television is now obsolete, as well as CRTs. The massive developments in \Gls{ic} fabrication led to the rise of the digital era we now live in. Therefore, most screens (be it televisions, monitors or cellphones) use digital, \gls{progressive}. As such, the use of analog compression techniques wasn't applicable. Accordingly, the evolution of digital video led to the development of digital compression techniques, such as the one presented in this work.

Being purely digital, these methodologies rely on computers and other processors to analyze data and apply the compression algorithms, making them very demanding processes from a computational standpoint. As expected, a high compression ratio is only obtainable by a high complexity algorithm, which also increases with the size of the video (more data leads to more analysis). Since in the early days of digital video, the used resolutions were lower as to the ones used in the present days, the compression algorithms used were not very demanding. However as the pursuit for higher quality video continued, so did the necessity for better compression ratios, and therefore the computational needs also increased. Such complex softwares lead to a high power consumption from the processor executing it, making such implementations unsuitable for portable, battery limited applications, such as cellphones or laptops. Besides this huge factor, such softwares tend to be very slow, specially when a real time compression or decompression is desired.

To amend for these factors, and to increase the reachability of high quality video to as many users as possible, these applications needed to have a viable solution that didn't compromise its usability. Accordingly, a new approach has been implemented on the most recent codec's. Besides the optimization of pure software compression/decompression solutions, there has been a great focus on the development of specialized hardware for such codecs. This solution could redress many of the problems presented previously, making them viable on a mobile implementation, as well as other specialized appliances, since such co-processors usually present a better performance than generic ones. This tendency has already been verified on the implementation choices on recent smartphones \cite[p.~14]{scientiamobileMobileOverviewReport}, as well as recent \textit{Nvidia} \gls{gpu} lineups \cite{VideoEncodeDecode2016}.

Since each compression algorithm tend to be very different from its predecessors, either by making changes to its bitstream or functioning principles, each time a new codec is released, there is a need to backup its development with a new set of hardware implementations. This makes the improvement of video compression techniques a continuous effort, in many engineering branches, as the technology needs to keep up with the demands of consumers, in a variety of applications.

Due to the broad access to video, and its influence in a variety of markets (besides video consumption itself), big companies have made investments on the improvement of video quality, and respective compression algorithms. These investments have provoked somewhat of a \emph{"Codec War"}. Since 2010, several video codecs have been deployed, and quickly replaced by a newer version, which presents better compression gains, at a lower quality degradation, such as the replace of \textit{VP8} (released in 2008) with \textit{VP9} (2013).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scope}

%\todo[inline,color=green!40]    {*AV1 is the most recent video codec}
%\todo[inline,color=green!40]    {*AV1 was developed by AOMedia}
%\todo[inline,color=green!40]    {*AV1 objective was to be an royalty free alternative to HEVC}

\emph{\gls{av1}} is the most recently released\footnote{Currently, there are other codecs being developed, without official bitstream release} video codec. It was developed as a Joint Development Foundation \cite{JointDevelopmentFoundation} project, under the name of \gls{aomedia} \footnote{Further explained in Chapter \ref{chap:av1}}. This codec took the same objective as its main predecessor, \textit{VP9}, which was to be an open source, royalty free alternative to \gls{mpeg}['s] state of the art video codec, \textit{\gls{HEVC}}.

Upon release, \textit{VP9} rivaled \textit{HEVC's} performance. However, soon after, the market demanded higher compression performance, giving origin to consortium of  enterprises that now represent AOM, and to the development of \textit{AV1}, in 2015. The first release of this coding format was made in March 2018, with the first release of its reference software, \textit{\gls{libaom}}, being made three months later, in June 2018.

Besides its main objectives, \textit{AV1} was also developed with the intent of being implementable in hardware. Therefore, various design choices were made to make the algorithm low memory consuming, and highly parallelizable. 

The desired compression performance was obtained at the cost of a highly complex algorithm (and reference software), that severely outperforms \textit{VP9}, at the cost of much higher compression times \cite{groisPerformanceComparisonAV12018}
\todo{referência para secção mais à frente?}
.

Taken these factors, there is a high demand for dedicated hardware architectures, that can speed up the compression/decompression times and reach real-time usability on live-streaming applications, such as video-conferencing, live-content visualization, etc.

\todo[inline,color=red!40]      {*Thesis objectives}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}

\todo[inline,color=red!40]{*General outline of the different chapters \textbf{Last section to do}}

\printbibliography[heading=subbibliography]
\addcontentsline{toc}{section}{References}