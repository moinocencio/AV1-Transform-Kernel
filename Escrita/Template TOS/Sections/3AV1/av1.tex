\cleardoublepage
\chapter{Video Compression Systems}\label{chap:av1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Principles}

\emph{Video Compression Systems} have been in development for approximately forty years, with the first video codec, \emph{H.120}, being released in 1984. It was composed of basic operations, which didn't correlate to good compression performances. This has lead to a quick downfall of its usage, being aggravated by the release of the \emph{H.261} standard by 1984.

However, the building blocks on which later standards were based are the same as in the first generations, i.e., the strategies implemented on newer standards exploit the same \emph{redundancies} as previous, less efficient, codecs.

By redundancies, it is meant disposable information to the playback of an image sequence. This concept is the key of video compression. Throughout the years, the enhancement of video codecs was based on the improving the algorithms which can reliably represent a video, while maintaining the least of the original information. In other words, the video sequence is analyzed for predictable/identifiable characteristics, e.g. the movement of a subject or the edge of an object, calculates strategies of predicting nearby pixel values through that information and disposes the unused information. This process is mentioned as \emph{redundancy removal}.

This way, to have a better understanding of the functioning behind video codecs, the mentioned redundancies, and respective origins, are presented. Most of such have origin on the way humans perceive vision, being this the first topic of this chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Human Visual System} \label{ssec:hvs}

%\todo[inline,color=green!40]{*Essency of video compression relies on making changes the image without serious perception by the user}
%\todo[inline,color=green!40]{*Eye Functioning}
%\todo[inline,color=red!40]{*"Known Issues" (lower perception to chroma, high frequencies, etc)}
%\todo[inline,color=red!40]{*Opportunity to explore various types of redundancies to the image}

\nocite{gonzalezDigitalImageProcessing2018}

Most of the compressed/decompressed video nowadays is directed to content visualization by consumers, with the exception of some network-driven image processing applications, such as automatic video surveillance. Therefore, the compression of video sequences has the intent of making changes to the original data, without serious impact to the users' perception. This process is mentioned as the removal of the \emph{Psychovisual redundancy} \cite{shiImageVideoCompression2008}. Therefore, a basic understanding of the visual system can clarify many of the design choices made in video compression applications, and why their use doesn't present much impact on the quality of the image, while greatly reducing its size.

The image perception starts in the human eye, represented in figure \ref{fig:eye}. Its different constituents accomplish different tasks, from focusing, to aperture control. Although their importance to the overall functioning of the eye, the part that matters most to the focus of this work is the innermost membrane, the retina.

\begin{figure}[h]
    \centering
    \includegraphics[width=\figwidth]{Sections/3AV1/Diagrams/eyediagram.png}
    \caption[Representation of an human eye]{Representation of an human eye \cite{AnatomyEyeHuman}}
    \label{fig:eye}
\end{figure}

Once the desired image is properly focused by the lens, an inverse version of it is shined on the aforementioned membrane, which is covered by two types of light sensitive cells, the \emph{cones} and \emph{rods}, which transform the observable image into a series of pulses, that get subsequently processed.

The cones are highly sensitive to color, being responsible for the \emph{photonic} or \emph{bright-light} vision. There are three different types of cones, corresponding to the wavelength they are susceptible to. These are the \emph{S}, \emph{M} and \emph{L} cones, being sensitive to, approximately, the blue, green and red light, respectively, making a somewhat similar capture to the RGB color system.

On the other side, rods aren't stimulated by bright light, being more active on low illumination levels. This aspect makes them responsible for giving a rough overview of the field of view. This is called as \emph{scotopic} or \emph{dim-light} vision. These cells are spread more broadly across the retina, while to the cones, which is also observable in the number of cells (approximately 6 million cones, to 100 million rods).

From this, it's already observable that the human visual system is more sensitive to differences on the luminosity, than to the color of an object \cite{mullenContrastSensitivityHuman1985}, which is a starting point for compressing video, as will be shown later in this chapter. However, many other opportunities come from the processing of the nerve signals, and the \emph{psychovisual} perception that follows.

Although more sensitive to \emph{luminance}, there is a threshold to which the difference between two objects --- $\Delta I$ --- can't be discerned. This relation is mentioned as \emph{contrast sensitivity function}, which is roughly approximated with the \emph{Weber's Law}

\begin{equation}
    \frac{\Delta I}{I}\approx constant
\end{equation}

Analyzing this equation, it's possible to conclude that the darker an object is, the lower the difference in luminance needs to be to distinguish another object. Also, darker images tend to be more susceptible to compression artifacts.

Besides the luminance values, the spatial and temporal frequencies also represent an important role in the perception of such errors. 

The image \ref{fig:noise} gives an example of the dependency with spatial frequency. The first image \ref{subfig:noiseOri} represents the original image, which got added with \gls{wgn}, represented in figure \ref{subfig:noise}. As it is observable, these artifacts are less noticeable on the highly detailed areas (branches and leafs of the tree) than in the smooth ones (sky in the top right corner). The effect of \emph{Weber's law} is also observed if we analyze the effect that the white noise as in the bright sun area, when compared to the darker areas.

\begin{figure}[h]
    \centering
    \begin{subfigure}[c]{\textwidth}
        \centering
        \includegraphics[width=\figwidth]{Sections/3AV1/Diagrams/paisagemOri.jpg}
        \caption{Original Image \cite{FreepikDownloadNow}}
        \label{subfig:noiseOri}
    \end{subfigure}
    \begin{subfigure}[c]{\textwidth}
        \centering
        \includegraphics[width=\figwidth]{Sections/3AV1/Diagrams/paisagemNoise.jpg}
        \caption{Image with added WGN}
        \label{subfig:noise}
    \end{subfigure}
    \caption{Example of the effect of added noise on figure}
    \label{fig:noise}
\end{figure}

Temporal frequency dependency, although more challenging to exemplify, is easily understandable. On a sequence of frames with fast camera, or subject movements, the human eye doesn't have the ability to track details or other artifacts, while in slow moving scenes, it can easily identify errors.

These are some of the "\emph{flaws}" of the human visual system, that get exploited during the compression of video. However, other \emph{redundancies}, inherent from the captured images themselves contribute to the reduction of the video size, as will be described in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Redundancy Exploitation}

%\todo[inline,color=red!40]{*Types of redundancies (Temporal, Statistical and Coding)}
%\todo[inline,color=red!40]{*Color subsampling}
%\todo[inline,color=red!40]{*Intra-prediction}
%\todo[inline,color=red!40]{*Inter-prediction}
%\todo[inline,color=red!40]{*Transform and Quantization}
%\todo[inline,color=red!40]{*Entropy Coding}

Even though there are countless observable subjects and sceneries, it's unfair to think of a frame as a random sequence of pixels. Objects tend to represent clusters of pixels with roughly the same values, moving objects follow predictable directions, etc. Such characteristics represent redundancies that can be removed.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Spatial Redundancy} \label{sssec:spatred}

Spatial redundancy comes from the similarity between neighboring pixels, on one frame. This aspect is easily verified through the autocorrelation of an image, as will be shown in the following example.

Taking image \ref{subfig:noiseOri} and calculating its autocorrelation with various horizontal shifts, gives origin to the graph in image \ref{fig:autocorr}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Sections/3AV1/Diagrams/intracorr.eps}
    \caption{Autocorrelation of image \ref{subfig:noiseOri}, with horizontal shifts}
    \label{fig:autocorr}
\end{figure}

As it is observable, for shorter shifts, the normalized autocorrelation is very close to 1, since most of the de-correlation comes for the mismatching edges. Although this relation varies depending on the image, it's safe to assume that it is very similar for the majority of the cases.

Such study gives a promising opportunity for compression, since it means that most pixels can be predicted from its neighbors. This aspect as lead to what now is known as \emph{differential} or \emph{predictive} coding.

On a video compression system, the spatial redundancy is considered in the \emph{intra-prediction} block, which calculates pixels, or pixel blocks, through its surrounds.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Temporal Redundancy}

As expected, a series of consecutive frames, on the same subject, tend to be very similar between each other, especially if considered the $30$ or $60 fps$ desired nowadays. 

Making a similar analysis to what was made in section \ref{sssec:spatred}, a series of frames of the \emph{Stefan} sequence \cite{YUVSequences} was considered, and the cross correlation between the first and the following nine was calculated, giving origin to the graph in figure \ref{fig:crosscorr}.
\todor{verify graph generation}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Sections/3AV1/Diagrams/intercorr.eps}
    \caption{Cross-correlation between the first and following nice frames of the \emph{Stefan} sequence}
    \label{fig:crosscorr}
\end{figure}

Similarly to what happened in the previous example, the cross correlation between consecutive frames is very high. Even though for faster moving scenes this relation might not be as pronounced, its application on video coding greatly contributes to the compression verified in the latest codecs. 

The codec takes advantage of this redundancy in the \emph{inter-prediction} stage, which is composed by the \emph{Motion Estimation} (MC) and \emph{Motion Compensation} (ME) blocks. On this stage, blocks of pixels in nearby frames are analyzed for movement, predicting its position for following frames.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Psychovisual Redundancy}

From the redundancies presented in section \ref{ssec:hvs}, video codecs implement compression measures in various stages.

The first measure is the \emph{chroma subsampling}, which takes advantage of the lower perception to color, discarding some of the \emph{chroma} samples, depending on the subsampling chosen.

Typically, a pixel value is represented in one luminance and two chrominance values, on the \emph{YCbCr} color space. The subsampling is defined in through the relation of luminance to chroma samples, being the most common the 4:4:4, 4:2:2 and 4:2:0 standards, represented in figure \ref{fig:subsample}. In the first one, no chroma samples are discarded, which means that for each 4 luminance (\emph{Y}) samples, there are 4 \emph{Cb} and 4 \emph{Cr} samples. Correspondingly, in the second standard, for each 4 Y samples, only 2 of each color components are maintained. The last example, although its misleading term, means that only 1 of each 4 chroma samples are kept.

\begin{figure}[h]
    \centering 
        \begin{subfigure}[c]{\textwidth}
            \centering
            \input{Sections/3AV1/Diagrams/sub444.tex}
            \caption{4:4:4}
            \label{subfig:444}
        \end{subfigure}
        \begin{subfigure}[c]{\textwidth}
            \centering
            \input{Sections/3AV1/Diagrams/sub422.tex}
            \caption{4:2:2}
            \label{subfig:422}
        \end{subfigure}
        \begin{subfigure}[c]{\textwidth}
            \centering
            \input{Sections/3AV1/Diagrams/sub420.tex}
            \caption{4:2:0}
            \label{subfig:420}
        \end{subfigure}
       \caption{Representation of chroma subsampling}
    \label{fig:subsample}
\end{figure}

From the reduced sensitivity to details (or areas with high spatial frequency), the compression is explored in the \emph{Transform} (T) and \emph{Quantization} (Q) blocks. In the first stage, blocks of pixels are evaluated in their frequency components. These are then evaluated in the second stage, where the least significant ones get discarded. In the decoder, the image is reconstructed with the maintained coefficients, without much impact to the image quality. This process is further explained throughout the work.

On the Quantization block, some work was also developed to account for \emph{Weber's law}, where the quantization depends on the average luminance value of the block. This concept was first introduced in \cite{watsonEfficiencyModelHuman1988}, and since then experimented in various codecs, such as HEVC \cite{rouisPerceptualVideoContent2018}.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Coding Redundancy}

Coding redundancy is directed to the method of representing information in the digital domain, i.e., the bits themselves, and how they are organized.

It is known that symbol probability plays a major role in information compression, across a wide variety of branches, and video is no exception. Taking this into account, codecs take advantage of coding redundancy in the \emph{Entropy Encoder} stage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Basic Video Compression/Decompression System}

From the basic principles of the previously mentioned blocks, it is possible to integrate them into two complete compression --- \emph{Encoder} --- and decompression --- \emph{Decoder} --- modules.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Encoder Model}

The encoder's objective is to compress a video sequence, turning it into a readable \emph{encoded bitstream}. To do this, the previously presented strategies get implemented on a system based on the schematic of figure \ref{fig:basicenc}.

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/basicEnc.tex}
    \caption{Simplified Basic Encoder Model}
    \label{fig:basicenc}
\end{figure}

The encoding process starts on the \emph{Input Frame}, which can be of two types. \emph{I Frames} are encoded using only the information present in themselves, i.e., using only \emph{Intra Prediction/Coding}, while \emph{P Frames} may use predictive coding from previously encoded frames \footnote{Most video codecs allow that the encoding sequence isn't the same as the temporal sequence. This allows to reference frames displayed next to the one being encoded.}.

The input gets split into blocks, which get fed into the two main blocks of a video encoder: the \emph{Intra} and \emph{Inter} Prediction blocks.

The \emph{Intra Coding} block, as mentioned previously, deals with the spatial redundancy, by predicting the current block from the pixels above and to the left of its upper and left edges. The prediction may be done with various algorithms, ranging from calculating the average from the reference pixels, to replicating these according to a certain direction. One such example is presented in figure \ref{fig:intraex}, where pixels B through H get spread across a $4\times 4$ block, diagonally.

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/intraex.tex}
    \caption{Directional Intra-prediction example}
    \label{fig:intraex}
\end{figure}

Into the \emph{Inter Coding} block, go two inputs. The currently encoding block, as well as a bank of previously encoded frames, named \emph{Reference Frames}. Firstly, the frames inside the buffer get searched for blocks resembling the former input. Once found, this process generates a \emph{motion vector}, corresponding to the difference between the position of the block found in the reference frame, and the position of the currently encoding block, as shown in figure \ref{fig:interex}.

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/interex.tex}
    \caption{Inter-prediction example}
    \label{fig:interex}
\end{figure}

After the prediction stage, the chosen output between the two processes, i.e., the predicted block, gets subtracted by the current one, giving origin to the \emph{residue}. This corresponds to the pixel value differences between the original and predicted blocks. Lower \emph{residues} indicate more efficient prediction stages.

The next stage, the \emph{Forward Transform}, is the focus of this work. It takes the residue blocks, which may not be the same size of the prediction blocks, and evaluates them according to its spatial frequencies. Its output corresponds to a series of \emph{coefficients}, that are related to the similarity --- or \emph{correlation} --- between the input block and a series of \emph{basis images}. This process is further explained in chapter \ref{chap:trans}.

On the \emph{Quantization} stage, the coefficients calculated in \textbf{T} get scaled according to a \emph{Quantization Matrix}. This stage takes advantage of the eye's lower perception to high frequency details, and scales the higher frequency coefficients by a higher value, than the lower, more significant ones. In most of the transformed blocks, this leads that only a few low frequency components are maintained, while the others get nullified, since they are not relevant to the reconstruction of the image. Therefore, this stage is the the one that presents the higher loss, although the previously presented also introduce errors.

The wipe out of the least significant coefficients is particularly efficient when paired with the last stage before the output, the \emph{Entropy Encoder}. On this block, \textbf{Q}'s output block gets run sequentially via a \emph{zig-zag scan}, which first passes through the lower frequency coefficients, followed by the higher frequency ones. 

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/zigzag.tex}
    \caption{Demonstration of Zig-Zag Scan}
    \label{fig:zigzag}
\end{figure}

In most of the cases, this causes that the non-zero coefficients get read first, followed by a sequence of zeros. Such sequence benefits heavily of being encoded with \gls{vlc}, such as \emph{Huffman Tree Codes} or \Gls{cabac}. Off all the processes, this is the one that doesn't introduce further distortion into the encoded sequence, which is the reason it doesn't get included in the \emph{feedback loop}.

The intent of this loop is to get an exact same copy of the frame reconstructed in the decoder. This reconstructed frame gets used as the reference for intra-prediction, or gets put into the reference frame buffer to be used in a later inter-prediction process. 

The output of the encoder is the quantized coefficients, as well as the necessary information to recreate the encoded blocks, such as the type of prediction used, the transformation \emph{kernel} [see p.\pageref{par:kernel}], quantization matrix, et al. These encoding parameters are the choices made by the \emph{Control Unit}, which although represented by a block in figure \ref{fig:basicenc}, may not be a local process, independent from all others. 

Since \emph{H.264}, most video codecs standardize the decoding process, specifically, the allowed tools for compressing the video, and how to use them. This means that the encoding process is widely adaptable to the compression objectives, as long as the final product is a bitstream following the norms set on the codec's standard \cite{AV1BitstreamDecoding}. Therefore, the definition of a \emph{Control Unit} is ambiguous in this context, since such unit can simply represent a set of parameters to be used throughout the encoding process\footnote{One such example would be \emph{lossless} compression modes, which use very a concise conditions on each stage, in order to get the least distortion.}, or an algorithm that can change between the different capabilities of the codec, in order to achieve an objective, such as a specific distortion rate, or not surpass a maximum bit rate.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Decoder Model}

As expected, the decoder (figure \ref{fig:basicdec}) does the backwards operation of the encoder on figure \ref{fig:basicenc}. It starts by analyzing the bitstream, separating the control information from the encoded and quantized coefficients.

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/basicDec.tex}
    \caption{Simplified Basic Decoder Model}
    \label{fig:basicdec}
\end{figure}

Having the encoding choices performed by the encoder, the decoder returns the \emph{coding redundancy} to the quantized coefficients, on the \emph{Entropy Decoding} stage. This corresponds to a translation from the varying length code used in codification, back into the raw coefficients.

The \emph{Inverse Quantization} rescales the maintained coefficients, resulting from the previous \emph{Quantization} stage. With this, it is meant that the same quantization matrix used when dividing the transformed coefficients, in the encoder, is now multiplied by the quantized parameters. It must be kept that this operation does not output an exact copy of the transformation coefficients, as a lot of information is permanently lost in \textbf{Q}. This process can be seen in figure \ref{fig:quant}.

\begin{figure}[!htbp]
    \centering
    \input{Sections/3AV1/Diagrams/quant.tex}
    \caption{Processing of $\protect 4\times 4$ residue block from \emph{transformation} to restoring} 
    \label{fig:quant}
\end{figure}

As can also be seen in this figure, the \emph{Inverse Transform} converts the coefficients back into spatial coordinates, therefore getting the restored residue. To obtain the final approximation of the block being decoded, this residue must be added to the same predicted block from the encoder. To do so, the \emph{Intra} or \emph{Inter Prediction} stages act according to the choices made in the encoding process, as to regenerate this block.

In the decoder, the \emph{Control Unit} represents the process that organizes the different stages, according to the choices done in the encoding stage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AV1}

\todo[inline,color=red!70]{\textbf{Review \textit{AV1 Bitstream and Decoding Process}}}
\todo[inline,color=red!40]{*Development Process}
\todo[inline,color=red!40]{*AOMedia companies}
\todo[inline,color=red!40]{*Comparation with past generations}
\todo[inline,color=red!40]{*Introduction of modules not present on other video codecs}
\todo[inline,color=red!40]{*Block diagram}

\input{Sections/3AV1/Diagrams/av1block.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Analysis}

\todo[inline,color=red!40]{*Compression gains}
\todo[inline,color=red!40]{*Quality assessment}
\todo[inline,color=red!40]{*Complexity (general/modules) and timing issues}

\clearpage
\printbibliography[heading=subbibliography]
\addcontentsline{toc}{section}{References}